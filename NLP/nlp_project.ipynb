{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone Project 2\n",
    "\n",
    "### Problem Statement\n",
    "The number of RCT papers released is continuously increasing. It is difficult for researchers to keep up with the latest research in their field. This project aims to create a tool that can help researchers to keep up with the latest research in their field. The tool will be able to search for the latest RCT papers in a specific field and summarize the key findings of the papers.\n",
    "\n",
    "### Solution\n",
    "Create an NLP model to classify abstract sentences into the role they play (e.g. objective, methods, results, etc)  to enable researchers to skim through the literature (hence SkimLit ðŸ¤“ðŸ”¥) and dive deeper when necessary.\n",
    "\n",
    "> ðŸ“– **Resources:** Before going through the code in this notebook, you might want to get a background of what we're going to be doing. To do so, spend an hour (or two) going through the following papers and then return to this notebook:\n",
    "1. Where our data is coming from: [*PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts*](https://arxiv.org/abs/1710.06071)\n",
    "2. Where our model is coming from: [*Neural networks for joint sentence\n",
    "classification in medical paper abstracts*](https://arxiv.org/pdf/1612.05251.pdf).\n",
    "\n",
    "## Topics to be covered\n",
    "* Downloading a dataset ([PubMed RCT200k from GitHub](https://github.com/Franck-Dernoncourt/pubmed-rct))\n",
    "* Preprocessing data for NLP models\n",
    "* Setting up a series of NLP models\n",
    "    * Making a baseline model (TF-IDF Multinomial Naive Bayes)\n",
    "    * Deep models with different combinations of layers (Conv1D, LSTM, GRU)\n",
    "* Building first multimodal model (feature extraction and token embeddings)\n",
    "    * Replicating the model architecture from https://arxiv.org/abs/1612.05251\n",
    "* Find the most wrong predictions\n",
    "* Making predictions on PubMed abstracts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "* `train.txt` - training samples\n",
    "* `dev.txt` - validation samples\n",
    "* `test.txt` - test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat by using the 20k dataset\n",
    "data_dir = \"data/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
       " 'data/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt',\n",
       " 'data/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all of the filenames in the data directory\n",
    "import os\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to read the lines of the documents\n",
    "def get_lines(filename):\n",
    "    \"\"\"\n",
    "    Reads filename (a text filename) and returns the lines of text as a list.\n",
    "    Args:\n",
    "    filename: a string containing the target text filename.\n",
    "    Returns:\n",
    "    A list of strings with one string per line from the target text filename.\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as file:\n",
    "        return file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
       " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
       " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
       " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
       " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
       " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
       " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
       " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
       " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n",
       " 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n",
       " 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n",
       " 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n",
       " '\\n',\n",
       " '###24854809\\n',\n",
       " 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n",
       " 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n",
       " 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n",
       " 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n",
       " 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines = get_lines(data_dir + \"train.txt\")\n",
    "train_lines[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to read the lines of the documents\n",
    "def preprocess_text_with_line_numbers(filename):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries of abstract line data.\n",
    "    \n",
    "    Takes in filename, reads its contents and sorts through each line,\n",
    "    extracting things like the line number, the text of the sentence,\n",
    "    and the target label.\n",
    "\n",
    "    Args:\n",
    "    filename: a string containing the target text filename.\n",
    "\n",
    "    Returns:\n",
    "    A list of dictionaries each containing the key value pairs for\n",
    "    line_number, target, and text.\n",
    "    \"\"\"\n",
    "\n",
    "    input_lines = get_lines(filename) # get all lines from filename\n",
    "    abstract_lines = \"\" # create an empty abstract\n",
    "    abstract_samples = [] # create an empty list to hold abstracts\n",
    "\n",
    "    # Loop through each line in the target file\n",
    "    for line in input_lines:\n",
    "        if line.startswith(\"###\"): # check to see if line is an ID line\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\" # reset abstract string\n",
    "        elif line.isspace(): # check to see if line is a new line\n",
    "            abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
    "\n",
    "            # Iterate through each line in abstract and count them at the same time\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                line_data = {} # create an empty dictionary for each line\n",
    "                target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
    "                line_data[\"target\"] = target_text_split[0] # get target label\n",
    "                line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
    "                line_data[\"line_number\"] = abstract_line_number # what line number does the line appear in the abstract?\n",
    "                line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstarct? (start from 0)\n",
    "                abstract_samples.append(line_data) # add line data to absract samples list\n",
    "\n",
    "        else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
    "                abstract_lines += line\n",
    "\n",
    "    return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data from file and preprocess it\n",
    "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\")\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
    "len(train_samples), len(val_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 'OBJECTIVE',\n",
       "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'these differences remained significant at @ weeks .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 11}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text  line_number  \\\n",
       "0  OBJECTIVE  to investigate the efficacy of @ weeks of dail...            0   \n",
       "1    METHODS  a total of @ patients with primary knee oa wer...            1   \n",
       "2    METHODS  outcome measures included pain reduction and i...            2   \n",
       "3    METHODS  pain was assessed using the visual analog pain...            3   \n",
       "4    METHODS  secondary outcome measures included the wester...            4   \n",
       "\n",
       "   total_lines  \n",
       "0           11  \n",
       "1           11  \n",
       "2           11  \n",
       "3           11  \n",
       "4           11  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of labels in the training data\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.total_lines.plot.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of sentences\n",
    "\n",
    "Can get a list of sentences form the DataFrame by calling the `tolist()` method on the `'text'` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert abstract text lines into lists\n",
    "train_sentences = train_df[\"text\"].tolist()\n",
    "val_sentences = val_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       " 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       " 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       " 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       " 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       " 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       " 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       " 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       " 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       " 'these differences remained significant at @ weeks .']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veiw the first 10 lines of the training sentences\n",
    "train_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make numeric labels (ML models require numeric labels)\n",
    "\n",
    "* Use sklearn's [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) and [`LabelEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) classes to transform labels (the different role types) into numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encode labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# check training labels\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 4, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the labels and encode them into integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
    "\n",
    "# check the training labels\n",
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a series of model experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Getting a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tf-idf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(X=train_sentences, y=train_labels_encoded);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7218323844829869"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the baseline on validation dataset\n",
    "model_0.score(X=val_sentences, y=val_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 3, ..., 4, 4, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics\n",
    "* Accuracy - the percent of sentences correctly classified\n",
    "* Precision - the percent of true role predictions out of all predictions\n",
    "* Recall - the percent of true role predictions out of all true role instances\n",
    "* F1 score - the harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate a model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a function to import an image and resize it to be able to be used with our model\n",
    "def load_and_prep_image(filename, img_shape=224, scale=True):\n",
    "  \"\"\"\n",
    "  Reads in an image from filename, turns it into a tensor and reshapes into\n",
    "  (224, 224, 3).\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  filename (str): string filename of target image\n",
    "  img_shape (int): size to resize target image to, default 224\n",
    "  scale (bool): whether to scale pixel values to range(0, 1), default True\n",
    "  \"\"\"\n",
    "  # Read in the image\n",
    "  img = tf.io.read_file(filename)\n",
    "  # Decode it into a tensor\n",
    "  img = tf.image.decode_jpeg(img)\n",
    "  # Resize the image\n",
    "  img = tf.image.resize(img, [img_shape, img_shape])\n",
    "  if scale:\n",
    "    # Rescale the image (get all values between 0 and 1)\n",
    "    return img/255.\n",
    "  else:\n",
    "    return img\n",
    "\n",
    "# Note: The following confusion matrix code is a remix of Scikit-Learn's \n",
    "# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Our function needs a different name to sklearn's plot_confusion_matrix\n",
    "def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): \n",
    "  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n",
    "\n",
    "  If classes is passed, confusion matrix will be labelled, if not, integer class values\n",
    "  will be used.\n",
    "\n",
    "  Args:\n",
    "    y_true: Array of truth labels (must be same shape as y_pred).\n",
    "    y_pred: Array of predicted labels (must be same shape as y_true).\n",
    "    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n",
    "    figsize: Size of output figure (default=(10, 10)).\n",
    "    text_size: Size of output figure text (default=15).\n",
    "    norm: normalize values or not (default=False).\n",
    "    savefig: save confusion matrix to file (default=False).\n",
    "  \n",
    "  Returns:\n",
    "    A labelled confusion matrix plot comparing y_true and y_pred.\n",
    "\n",
    "  Example usage:\n",
    "    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n",
    "                          y_pred=y_preds, # predicted labels\n",
    "                          classes=class_names, # array of class label names\n",
    "                          figsize=(15, 15),\n",
    "                          text_size=10)\n",
    "  \"\"\"  \n",
    "  # Create the confustion matrix\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n",
    "  n_classes = cm.shape[0] # find the number of classes we're dealing with\n",
    "\n",
    "  # Plot the figure and make it pretty\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n",
    "  fig.colorbar(cax)\n",
    "\n",
    "  # Are there a list of classes?\n",
    "  if classes:\n",
    "    labels = classes\n",
    "  else:\n",
    "    labels = np.arange(cm.shape[0])\n",
    "  \n",
    "  # Label the axes\n",
    "  ax.set(title=\"Confusion Matrix\",\n",
    "         xlabel=\"Predicted label\",\n",
    "         ylabel=\"True label\",\n",
    "         xticks=np.arange(n_classes), # create enough axis slots for each class\n",
    "         yticks=np.arange(n_classes), \n",
    "         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n",
    "         yticklabels=labels)\n",
    "  \n",
    "  # Make x-axis labels appear on bottom\n",
    "  ax.xaxis.set_label_position(\"bottom\")\n",
    "  ax.xaxis.tick_bottom()\n",
    "\n",
    "  # Set the threshold for different colors\n",
    "  threshold = (cm.max() + cm.min()) / 2.\n",
    "\n",
    "  # Plot the text on each cell\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    if norm:\n",
    "      plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n",
    "              horizontalalignment=\"center\",\n",
    "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "              size=text_size)\n",
    "    else:\n",
    "      plt.text(j, i, f\"{cm[i, j]}\",\n",
    "              horizontalalignment=\"center\",\n",
    "              color=\"white\" if cm[i, j] > threshold else \"black\",\n",
    "              size=text_size)\n",
    "\n",
    "  # Save the figure to the current working directory\n",
    "  if savefig:\n",
    "    fig.savefig(\"confusion_matrix.png\")\n",
    "  \n",
    "# Make a function to predict on images and plot them (works with multi-class)\n",
    "def pred_and_plot(model, filename, class_names):\n",
    "  \"\"\"\n",
    "  Imports an image located at filename, makes a prediction on it with\n",
    "  a trained model and plots the image with the predicted class as the title.\n",
    "  \"\"\"\n",
    "  # Import the target image and preprocess it\n",
    "  img = load_and_prep_image(filename)\n",
    "\n",
    "  # Make a prediction\n",
    "  pred = model.predict(tf.expand_dims(img, axis=0))\n",
    "\n",
    "  # Get the predicted class\n",
    "  if len(pred[0]) > 1: # check for multi-class\n",
    "    pred_class = class_names[pred.argmax()] # if more than one output, take the max\n",
    "  else:\n",
    "    pred_class = class_names[int(tf.round(pred)[0][0])] # if only one output, round\n",
    "\n",
    "  # Plot the image and predicted class\n",
    "  plt.imshow(img)\n",
    "  plt.title(f\"Prediction: {pred_class}\")\n",
    "  plt.axis(False);\n",
    "  \n",
    "import datetime\n",
    "\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "  \"\"\"\n",
    "  Creates a TensorBoard callback instand to store log files.\n",
    "\n",
    "  Stores log files with the filepath:\n",
    "    \"dir_name/experiment_name/current_datetime/\"\n",
    "\n",
    "  Args:\n",
    "    dir_name: target directory to store TensorBoard log files\n",
    "    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
    "  \"\"\"\n",
    "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir\n",
    "  )\n",
    "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "  return tensorboard_callback\n",
    "\n",
    "# Plot the validation and training data separately\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss_curves(history):\n",
    "  \"\"\"\n",
    "  Returns separate loss curves for training and validation metrics.\n",
    "\n",
    "  Args:\n",
    "    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
    "  \"\"\" \n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  accuracy = history.history['accuracy']\n",
    "  val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "  epochs = range(len(history.history['loss']))\n",
    "\n",
    "  # Plot loss\n",
    "  plt.plot(epochs, loss, label='training_loss')\n",
    "  plt.plot(epochs, val_loss, label='val_loss')\n",
    "  plt.title('Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend()\n",
    "\n",
    "  # Plot accuracy\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "  plt.title('Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend();\n",
    "\n",
    "def compare_historys(original_history, new_history, initial_epochs=5):\n",
    "    \"\"\"\n",
    "    Compares two TensorFlow model History objects.\n",
    "    \n",
    "    Args:\n",
    "      original_history: History object from original model (before new_history)\n",
    "      new_history: History object from continued model training (after original_history)\n",
    "      initial_epochs: Number of epochs in original_history (new_history plot starts from here) \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get original history measurements\n",
    "    acc = original_history.history[\"accuracy\"]\n",
    "    loss = original_history.history[\"loss\"]\n",
    "\n",
    "    val_acc = original_history.history[\"val_accuracy\"]\n",
    "    val_loss = original_history.history[\"val_loss\"]\n",
    "\n",
    "    # Combine original history with new history\n",
    "    total_acc = acc + new_history.history[\"accuracy\"]\n",
    "    total_loss = loss + new_history.history[\"loss\"]\n",
    "\n",
    "    total_val_acc = val_acc + new_history.history[\"val_accuracy\"]\n",
    "    total_val_loss = val_loss + new_history.history[\"val_loss\"]\n",
    "\n",
    "    # Make plots\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(total_acc, label='Training Accuracy')\n",
    "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
    "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
    "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(total_loss, label='Training Loss')\n",
    "    plt.plot(total_val_loss, label='Validation Loss')\n",
    "    plt.plot([initial_epochs-1, initial_epochs-1],\n",
    "              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "  \n",
    "# Create function to unzip a zipfile into current working directory \n",
    "# (since we're going to be downloading and unzipping a few files)\n",
    "import zipfile\n",
    "\n",
    "def unzip_data(filename):\n",
    "  \"\"\"\n",
    "  Unzips filename into the current working directory.\n",
    "\n",
    "  Args:\n",
    "    filename (str): a filepath to a target zip folder to be unzipped.\n",
    "  \"\"\"\n",
    "  zip_ref = zipfile.ZipFile(filename, \"r\")\n",
    "  zip_ref.extractall()\n",
    "  zip_ref.close()\n",
    "\n",
    "# Walk through an image classification directory and find out how many files (images)\n",
    "# are in each subdirectory.\n",
    "import os\n",
    "\n",
    "def walk_through_dir(dir_path):\n",
    "  \"\"\"\n",
    "  Walks through dir_path returning its contents.\n",
    "\n",
    "  Args:\n",
    "    dir_path (str): target directory\n",
    "  \n",
    "  Returns:\n",
    "    A print out of:\n",
    "      number of subdiretories in dir_path\n",
    "      number of images (files) in each subdirectory\n",
    "      name of each subdirectory\n",
    "  \"\"\"\n",
    "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
    "    \n",
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def calculate_results(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "\n",
    "  Args:\n",
    "      y_true: true labels in the form of a 1D array\n",
    "      y_pred: predicted labels in the form of a 1D array\n",
    "\n",
    "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \"\"\"\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score using \"weighted average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "  return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 72.1832384482987,\n",
       " 'precision': 0.7186466952323352,\n",
       " 'recall': 0.7218323844829869,\n",
       " 'f1': 0.6989250353450294}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the baseline results\n",
    "baseline_results = calculate_results(y_true=val_labels_encoded, y_pred=baseline_preds)\n",
    "\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for deep sequence models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.338269273494777"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avarege sentence length\n",
    "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_len = np.mean(sent_lens)\n",
    "avg_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sent_lens, bins=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq_len = int(np.percentile(sent_lens, 95)) # 95% of sequences should be shorter than this length\n",
    "output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximum number of words in a sequence\n",
    "max(sent_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create text vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 68000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Create a text vectorizer layer\n",
    "# text_vectorizer = TextVectorization(max_tokens=max_tokens, # number of words in vocabulary\n",
    "#                                     output_sequence_length=55)\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "text_vectorizer = TextVectorization(max_tokens=max_tokens, # number of words in vocabulary\n",
    "                                    output_sequence_length=output_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = (train_dataset.map(lambda x, y: (text_vectorizer(x), y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "at @ weeks of gestation , @ ( @ % ) women were classified as emotional eaters , @ ( @ % ) as external eaters and @ ( @ % ) as restrained eaters .\n",
      "\n",
      "Length of text: 36\n",
      "\n",
      "Vectorized text:\n",
      "[[   15    53     4  1443    90     9  2102    25  1413 15020    25  1489\n",
      "  15020     3    25 21919 15020     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Test text vectorizer on random sentences\n",
    "import random\n",
    "\n",
    "# Adapt the text vectorizer to the training sentences\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "target_sentence = random.choice(train_sentences)\n",
    "print(f\"Text:\\n{target_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
    "print(f\"\\nVectorized text:\\n{text_vectorizer([target_sentence])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabu;ary: 64841\n",
      "Most common words in the vocabulary: ['', '[UNK]', 'the', 'and', 'of']\n",
      "Least common words in the vocabulary: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "source": [
    "# Number of words in the vocabulary\n",
    "rct_20k_text_vocb = text_vectorizer.get_vocabulary()\n",
    "print(f\"Number of words in vocabu;ary: {len(rct_20k_text_vocb)}\")\n",
    "print(f\"Most common words in the vocabulary: {rct_20k_text_vocb[:5]}\")\n",
    "print(f\"Least common words in the vocabulary: {rct_20k_text_vocb[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None,),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 55,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the config of the text vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence before vectorization:\n",
      "at @ weeks of gestation , @ ( @ % ) women were classified as emotional eaters , @ ( @ % ) as external eaters and @ ( @ % ) as restrained eaters .\n",
      "\n",
      "Sentence after vectorization (before embedding):\n",
      " [[   15    53     4  1443    90     9  2102    25  1413 15020    25  1489\n",
      "  15020     3    25 21919 15020     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n",
      "\n",
      "Sentence after embedding:\n",
      "[[[-0.01276223 -0.00749974 -0.02712342 ... -0.01898676 -0.01900991\n",
      "   -0.03605894]\n",
      "  [ 0.00359537  0.04771153 -0.00643243 ... -0.01527812 -0.0119317\n",
      "   -0.02938175]\n",
      "  [ 0.0169819   0.03483384  0.03124685 ...  0.04920616 -0.02795278\n",
      "    0.04448627]\n",
      "  ...\n",
      "  [-0.02169852  0.03183036  0.01232423 ... -0.00262802 -0.03790481\n",
      "    0.04838889]\n",
      "  [-0.02169852  0.03183036  0.01232423 ... -0.00262802 -0.03790481\n",
      "    0.04838889]\n",
      "  [-0.02169852  0.03183036  0.01232423 ... -0.00262802 -0.03790481\n",
      "    0.04838889]]]\n",
      "\n",
      "Embedded sentence shape: (1, 55, 128)\n"
     ]
    }
   ],
   "source": [
    "# Create token embedding layer\n",
    "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocb),# length of the vocabulary\n",
    "                               output_dim=128,\n",
    "                               mask_zero=True, # use masking to handle variable sequence lengths\n",
    "                               name=\"token_embedding\")\n",
    "\n",
    "# example of token embedding\n",
    "print(f\"Sentence before vectorization:\\n{target_sentence}\\n\")\n",
    "vectorized_sentence = text_vectorizer([target_sentence])\n",
    "print(f\"Sentence after vectorization (before embedding):\\n {vectorized_sentence}\\n\")\n",
    "embedded_sentence = token_embed(vectorized_sentence)\n",
    "print(f\"Sentence after embedding:\\n{embedded_sentence}\\n\")\n",
    "print(f\"Embedded sentence shape: {embedded_sentence.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets (inputs and targets) for deep sequence models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_encoded))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_encoded))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_encoded))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Conv1D with token embeddings\n",
    "\n",
    "```\n",
    "Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create 1D convolutional model to process sequences\n",
    "# inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "# text_vectors = text_vectorizer(inputs)\n",
    "# token_embeddings = token_embed(text_vectors)\n",
    "# x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings)\n",
    "# x = layers.GlobalAveragePooling1D()(x)\n",
    "# outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "# model_1 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# # Compile the model\n",
    "# model_1.compile(loss=\"categorical_crossentropy\",\n",
    "#                 optimizer=tf.keras.optimizers.Adam(),\n",
    "#                 metrics=[\"accracy\"])\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
    "token_embeddings = token_embed(text_vectors) # create embedding\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile\n",
    "model_1.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 55)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " token_embedding (Embedding)  (None, 55, 128)          8299648   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 55, 64)            41024     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,340,997\n",
      "Trainable params: 8,340,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 12s 12ms/step - loss: 0.9252 - accuracy: 0.6332 - val_loss: 0.6853 - val_accuracy: 0.7427\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 7s 12ms/step - loss: 0.6590 - accuracy: 0.7563 - val_loss: 0.6270 - val_accuracy: 0.7726\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 7s 12ms/step - loss: 0.6186 - accuracy: 0.7738 - val_loss: 0.5969 - val_accuracy: 0.7859\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the training dataset by applying one-hot encoding\n",
    "train_dataset = train_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes)))\n",
    "\n",
    "# Preprocess the validation dataset by applying one-hot encoding\n",
    "valid_dataset = valid_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes)))\n",
    "\n",
    "# Fit the model with preprocessed datasets\n",
    "model_1_history = model_1.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=int(0.1 * len(train_dataset)),  # only fit on 10% of batches\n",
    "    epochs=3,\n",
    "    validation_data=valid_dataset,\n",
    "    validation_steps=int(0.1 * len(valid_dataset))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# model_1_history = model_1.fit(train_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes))),\n",
    "#                               steps_per_epoch=int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training time\n",
    "#                               epochs=3,\n",
    "#                               validation_data=valid_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes))),\n",
    "#                               validation_steps=int(0.1 * len(valid_dataset))) # only validate on 10% of batches\n",
    "# model_1_history = model_1.fit(train_dataset,\n",
    "#                               steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "#                               epochs=3,\n",
    "#                               validation_data=valid_dataset,\n",
    "#                               validation_steps=int(0.1 * len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 6s 6ms/step - loss: 0.5989 - accuracy: 0.7856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5988991856575012, 0.7856481075286865]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the data\n",
    "model_1.evaluate(valid_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 3s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.0590879e-01, 1.8631808e-01, 8.3553538e-02, 2.9013267e-01,\n",
       "        3.4086905e-02],\n",
       "       [4.5049539e-01, 2.2682874e-01, 1.0897840e-02, 3.0601332e-01,\n",
       "        5.7647019e-03],\n",
       "       [1.6243978e-01, 8.2886862e-03, 1.6161182e-03, 8.2760090e-01,\n",
       "        5.4590429e-05],\n",
       "       ...,\n",
       "       [3.2637324e-06, 5.9843209e-04, 5.0773280e-04, 2.0319433e-06,\n",
       "        9.9888855e-01],\n",
       "       [6.0691774e-02, 4.2431560e-01, 1.1564250e-01, 6.6584036e-02,\n",
       "        3.3276612e-01],\n",
       "       [1.7372678e-01, 6.4728868e-01, 5.6867190e-02, 3.7043057e-02,\n",
       "        8.5074328e-02]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "model_1_pred_probs = model_1.predict(valid_dataset)\n",
    "model_1_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the prediction probabilities to class labels\n",
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.56480868529061,\n",
       " 'precision': 0.7821564725958795,\n",
       " 'recall': 0.7856480868529061,\n",
       " 'f1': 0.7832661306998852}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the model_1 results\n",
    "model_1_results = calculate_results(y_true=val_labels_encoded, y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
